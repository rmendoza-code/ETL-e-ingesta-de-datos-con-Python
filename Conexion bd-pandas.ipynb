{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "advance-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "departmental-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conexion\n",
    "con = sqlite3.connect(\"C:/Users/babst/Downloads/nba_salary.sqlite\")\n",
    "\n",
    "#QUERY\n",
    "sql = \"SELECT name from sqlite_master where type = 'table'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "going-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear dt\n",
    "df = pd.read_sql_query(sql,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "solid-medline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name\n",
      "0  NBA_season1718_salary\n",
      "1          Seasons_Stats\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "twelve-technology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        X1          Player   Tm  season17_18\n",
      "0      1.0   Stephen Curry  GSW   34682550.0\n",
      "1      2.0    LeBron James  CLE   33285709.0\n",
      "2      3.0    Paul Millsap  DEN   31269231.0\n",
      "3      4.0  Gordon Hayward  BOS   29727900.0\n",
      "4      5.0   Blake Griffin  DET   29512900.0\n",
      "..     ...             ...  ...          ...\n",
      "568  569.0      Quinn Cook  NOP      25000.0\n",
      "569  570.0   Chris Johnson  HOU      25000.0\n",
      "570  571.0      Beno Udrih  DET      25000.0\n",
      "571  572.0   Joel Bolomboy  MIL      22248.0\n",
      "572  573.0    Jarell Eddie  CHI      17224.0\n",
      "\n",
      "[573 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_nba = pd.read_sql_query(\"Select * From NBA_season1718_salary\",con)\n",
    "print(df_nba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bizarre-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQLAlchemy\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lesbian-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conexion\n",
    "engine = db.create_engine('sqlite:///C:/Users/babst/Downloads/nba_salary.sqlite')\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bigger-cement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            X1    Year             Player  Pos   Age   Tm     G   GS      MP  \\\n",
      "0          0.0  1950.0    Curly Armstrong  G-F  31.0  FTW  63.0  NaN     NaN   \n",
      "1          1.0  1950.0       Cliff Barker   SG  29.0  INO  49.0  NaN     NaN   \n",
      "2          2.0  1950.0      Leo Barnhorst   SF  25.0  CHS  67.0  NaN     NaN   \n",
      "3          3.0  1950.0         Ed Bartels    F  24.0  TOT  15.0  NaN     NaN   \n",
      "4          4.0  1950.0         Ed Bartels    F  24.0  DNN  13.0  NaN     NaN   \n",
      "...        ...     ...                ...  ...   ...  ...   ...  ...     ...   \n",
      "24686  24686.0  2017.0        Cody Zeller   PF  24.0  CHO  62.0  NaN  1725.0   \n",
      "24687  24687.0  2017.0       Tyler Zeller    C  27.0  BOS  51.0  NaN   525.0   \n",
      "24688  24688.0  2017.0  Stephen Zimmerman    C  20.0  ORL  19.0  0.0   108.0   \n",
      "24689  24689.0  2017.0        Paul Zipser   SF  22.0  CHI  44.0  NaN   843.0   \n",
      "24690  24690.0  2017.0        Ivica Zubac    C  19.0  LAL  38.0  NaN   609.0   \n",
      "\n",
      "        PER  ...    FT%  ORB  DRB    TRB    AST  STL  BLK  TOV     PF    PTS  \n",
      "0       NaN  ...  0.705  NaN  NaN    NaN  176.0  NaN  NaN  NaN  217.0  458.0  \n",
      "1       NaN  ...  0.708  NaN  NaN    NaN  109.0  NaN  NaN  NaN   99.0  279.0  \n",
      "2       NaN  ...  0.698  NaN  NaN    NaN  140.0  NaN  NaN  NaN  192.0  438.0  \n",
      "3       NaN  ...  0.559  NaN  NaN    NaN   20.0  NaN  NaN  NaN   29.0   63.0  \n",
      "4       NaN  ...  0.548  NaN  NaN    NaN   20.0  NaN  NaN  NaN   27.0   59.0  \n",
      "...     ...  ...    ...  ...  ...    ...    ...  ...  ...  ...    ...    ...  \n",
      "24686  16.7  ...  0.679  NaN  NaN  405.0   99.0  NaN  NaN  NaN  189.0  639.0  \n",
      "24687  13.0  ...  0.564  NaN  NaN  124.0   42.0  NaN  NaN  NaN   61.0  178.0  \n",
      "24688   7.3  ...  0.600  NaN  NaN   35.0    4.0  NaN  NaN  NaN   17.0   23.0  \n",
      "24689   6.9  ...  0.775  NaN  NaN  125.0   36.0  NaN  NaN  NaN   78.0  240.0  \n",
      "24690  17.0  ...  0.653  NaN  NaN  159.0   30.0  NaN  NaN  NaN   66.0  284.0  \n",
      "\n",
      "[24691 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_nba_sqlalchemy = pd.read_sql_query(\"Select * from Seasons_Stats\",connection)\n",
    "print(df_nba_sqlalchemy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "distinguished-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consultas!\n",
    "df_Curly = pd.read_sql_query(\"SELECT * from Seasons_Stats where Player = 'Curly Armstrong'\",connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "central-ocean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>Year</th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>PER</th>\n",
       "      <th>...</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>Curly Armstrong</td>\n",
       "      <td>G-F</td>\n",
       "      <td>31.0</td>\n",
       "      <td>FTW</td>\n",
       "      <td>63.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>217.0</td>\n",
       "      <td>458.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>314.0</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>Curly Armstrong</td>\n",
       "      <td>G-F</td>\n",
       "      <td>32.0</td>\n",
       "      <td>FTW</td>\n",
       "      <td>38.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>89.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>97.0</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X1    Year           Player  Pos   Age   Tm     G    GS    MP   PER  \\\n",
       "0    0.0  1950.0  Curly Armstrong  G-F  31.0  FTW  63.0  None  None  None   \n",
       "1  314.0  1951.0  Curly Armstrong  G-F  32.0  FTW  38.0  None  None  None   \n",
       "\n",
       "   ...    FT%   ORB   DRB   TRB    AST   STL   BLK   TOV     PF    PTS  \n",
       "0  ...  0.705  None  None   NaN  176.0  None  None  None  217.0  458.0  \n",
       "1  ...  0.644  None  None  89.0   77.0  None  None  None   97.0  202.0  \n",
       "\n",
       "[2 rows x 53 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Curly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deluxe-summit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510.1163499025341"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prom_PTS = df_nba_sqlalchemy['PTS'].mean()\n",
    "prom_PTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "technological-helen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    100\n",
      "1    200\n",
      "2    300\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "serie = pd.Series([100, 200, 300])\n",
    "print(serie)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "square-breath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Product  Sales\n",
      "0       A    100\n",
      "1       B    200\n"
     ]
    }
   ],
   "source": [
    "data = {'Product': ['A', 'B'], 'Sales': [100, 200]}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "attractive-design",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   new_column\n",
      "0         100\n",
      "1         200\n",
      "2         300\n"
     ]
    }
   ],
   "source": [
    "df_nuevo = serie.to_frame(name='new_column')\n",
    "print(df_nuevo)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "peripheral-wellington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1        0\n",
       "Year      0\n",
       "Player    0\n",
       "Pos       0\n",
       "Age       0\n",
       "Tm        0\n",
       "G         0\n",
       "GS        2\n",
       "MP        2\n",
       "PER       2\n",
       "TS%       0\n",
       "3PAr      2\n",
       "FTr       0\n",
       "ORB%      2\n",
       "DRB%      2\n",
       "TRB%      2\n",
       "AST%      2\n",
       "STL%      2\n",
       "BLK%      2\n",
       "TOV%      2\n",
       "USG%      2\n",
       "blanl     2\n",
       "OWS       0\n",
       "DWS       0\n",
       "WS        0\n",
       "WS/48     2\n",
       "blank2    2\n",
       "OBPM      2\n",
       "DBPM      2\n",
       "BPM       2\n",
       "VORP      2\n",
       "FG        0\n",
       "FGA       0\n",
       "FG%       0\n",
       "3P        2\n",
       "3PA       2\n",
       "3P%       2\n",
       "2P        0\n",
       "2PA       0\n",
       "2P%       0\n",
       "eFG%      0\n",
       "FT        0\n",
       "FTA       0\n",
       "FT%       0\n",
       "ORB       2\n",
       "DRB       2\n",
       "TRB       1\n",
       "AST       0\n",
       "STL       2\n",
       "BLK       2\n",
       "TOV       2\n",
       "PF        0\n",
       "PTS       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Curly.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cutting-symposium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicados = df_Curly.duplicated()\n",
    "print(duplicados.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "intensive-baking",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date_column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1ab67321b2b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date_column'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalidate_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date_column'"
     ]
    }
   ],
   "source": [
    "df['date_column'].apply(lambda x: validate_date(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Leer Archivo CSV generado\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/babst/Downloads/datos_ejemplo.csv\")\n",
    "\n",
    "# Mostrar el df original\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Manejo de valores nulos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum()) #Verificar valores nulos\n",
    "\n",
    "#3 Eliminar las filas que contienen valores nulos\n",
    "df_sin_nulos = df.dropna()\n",
    "\n",
    "#4 Rellenar los valores nulos con un valor específico (por ejemplo, 0 o \"Desconocido\")\n",
    "df_rellenado = df.fillna({'Salario': 0, 'Nombre': 'Desconocido'})\n",
    "\n",
    "print(\"\\nDataFrame después de eliminar nulos:\")\n",
    "print(df_sin_nulos)\n",
    "\n",
    "print(\"\\nDataFrame después de rellenar nulos:\")\n",
    "print(df_rellenado) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Deteccióny Correción de errores en los tipos de datos\n",
    "\n",
    "# Intentar convertir la columna 'Edad' a numérica\n",
    "df['Edad'] = pd.to_numeric(df['Edad'], errors = 'coerce')\n",
    "\n",
    "# Ver los tipos de datos de las columnas antes de la correción\n",
    "print(df.dtypes)\n",
    "\n",
    "# Corregir valores no numéricos en 'Edad' (por ejemplo, convertir a NaN)\n",
    "df['Edad'] = df['Edad'].fillna(df['Edad'].mean()) #Rellenar NaN con el promedio de la columna\n",
    "\n",
    "# Ver los tipos de datos después de la correción\n",
    "print(df.dtypes)\n",
    "\n",
    "# Ver el df corregido\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Conversión de variables categóricas en números\n",
    "\n",
    "df['Genero'] = df['Genero'].map({'F': 0, 'M': 1}) \n",
    "\n",
    "# Usar la técnica de 'get_dummies' para convertir 'Departamento' en variables dummy\n",
    "df = pd.get_dummies(df, columns = ['Departamento'], drop_first = True)\n",
    "\n",
    "# Ver el DF final\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = df[df['Edad'] > 35]\n",
    "print(df_filtrado) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seleccion = df[['Edad', 'Salario']]\n",
    "print(df_seleccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nueva_columna'] = df['Edad'] * df['Salario']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'producto': ['A', 'B', 'C'],\n",
    "    'cantidad': [3, 0, 5],\n",
    "    'precio_unitario': [10, 20, 15]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = df[df['cantidad'] > 1]\n",
    "print(df_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seleccion = df_filtrado[['producto', 'cantidad']]\n",
    "print(df_seleccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado['total_venta'] = df_filtrado['cantidad'] * df_filtrado['precio_unitario']\n",
    "print(df_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar y Resumir Datos\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'producto': ['A', 'B', 'A', 'B'],\n",
    "    'ventas': [10, 20, 30, 40]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "grupo = df.groupby('producto').agg({'ventas': ['sum', 'mean']})\n",
    "print(grupo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#1. Importar archivo CSV de empleados  y bonificaciones\n",
    "df_empleados = pd.read_csv(\"C:/Users/babst/Downloads/empleados.csv\")\n",
    "df_bonificaciones = pd.read_csv(\"C:/Users/babst/Downloads/bonificaciones.csv\")\n",
    "\n",
    "#2. Mostrar DataFrames\n",
    "print(df_empleados)\n",
    "print(df_bonificaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Aplicar funciones personalizadas\n",
    "\n",
    "# Definir una función personalizada para calcular el salario anualizado\n",
    "def salario_anual(salario):\n",
    "    return salario * 12\n",
    "\n",
    "# Aplicar la función personalizada a la columna 'Salario'\n",
    "df_empleados['Salario_anual'] = df_empleados['Salario'].apply(salario_anual)\n",
    "print(df_empleados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Uso de apply()  para aplicar funciones más complejas\n",
    "\n",
    "# Crear una función que calculesi un empleado tiene más de 5 años de antiguüedad\n",
    "def antiguedad_5anos(fecha_ingreso):\n",
    "    today = pd.to_datetime('today')\n",
    "    antiguedad = today - pd.to_datetime(fecha_ingreso)\n",
    "    return antiguedad.days / 365 > 5\n",
    "\n",
    "df_empleados['Antiguedad_mayor_5'] = df_empleados['Fecha_Ingreso'].apply(antiguedad_5anos)\n",
    "print(df_empleados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Crear tablas pivote (Pivot Tables)\n",
    "\n",
    "# Crear una tabla pivote que muestre el salario promedio por departamento\n",
    "pivot_departamento = df_empleados.pivot_table(values = 'Salario', index = 'Departamento', aggfunc = 'mean')\n",
    "\n",
    "print(pivot_departamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Merge y Join para combinar DataFrames\n",
    "\n",
    "# Realizar un merge entre los dos DataFrames usando la columna 'ID_empleado'\n",
    "df_completo = pd.merge(df_empleados, df_bonificaciones, on = 'ID_Empleado', how = 'left')\n",
    "\n",
    "print(df_completo)\n",
    " \n",
    "    \n",
    "# Usar Join:    \n",
    "# Crear dataframe adicional de departamentos con la información de ubicación \n",
    "data_departamentos = {\n",
    "    'Departamento': ['Ventas', 'TI', 'Marketing'],\n",
    "    'Ubicación': ['Madrid','Barcelona', 'Valencia']\n",
    "}\n",
    "\n",
    "df_departamentos = pd.DataFrame(data_departamentos)\n",
    "\n",
    "# Establecer 'Departamento' como índice del DataFrame de departamentos\n",
    "df_departamentos.set_index('Departamento', inplace = True)\n",
    "\n",
    "#Realizar un join entre el DataFrame de empleados y el de departamentos usando la columna 'Departamento'\n",
    "df_join = df_empleados.set_index('Departamento').join(df_departamentos)\n",
    "\n",
    "print(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "silent-torture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Producto  Precio  Cantidad\n",
      "0    Camisa    20.0         2\n",
      "1  Pantalón    40.0         3\n",
      "2   Zapatos    50.0         1\n",
      "3  Sudarera    60.0         2\n",
      "4  Sombrero    15.0         5\n",
      "   Producto  Precio  Cantidad  Total_ventas\n",
      "0    Camisa    20.0         2          40.0\n",
      "1  Pantalón    40.0         3         120.0\n",
      "2   Zapatos    50.0         1          50.0\n",
      "3  Sudarera    60.0         2         120.0\n",
      "4  Sombrero    15.0         5          75.0\n",
      "   Producto  Precio  Cantidad  Total_ventas Clasificación_producto\n",
      "0    Camisa    20.0         2          40.0                   Caro\n",
      "1  Pantalón    40.0         3         120.0                  Medio\n",
      "2   Zapatos    50.0         1          50.0                   Caro\n",
      "3  Sudarera    60.0         2         120.0                   Caro\n",
      "4  Sombrero    15.0         5          75.0                 Barato\n"
     ]
    }
   ],
   "source": [
    "# Crear DataFrame \n",
    "data = {\n",
    "    'Producto': ['Camisa', 'Pantalón', 'Zapatos', 'Sudarera', 'Sombrero'],\n",
    "    'Precio': [20.0, 40.0 ,50.0 ,60.0 ,15.0],\n",
    "    'Cantidad': [2, 3, 1, 2, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "def antiguedad_5anos(fecha_ingreso):\n",
    "    today = pd.to_datetime('today')\n",
    "    antiguedad = today - pd.to_datetime(fecha_ingreso)\n",
    "    return antiguedad.days / 365 > 5\n",
    "\n",
    "# Definir función\n",
    "def total_ventas(precio):\n",
    "    indice = df[df['Precio'] == precio].index[0]\n",
    "    cantidad = df.loc[indice, 'Cantidad']\n",
    "    return precio * cantidad\n",
    "\n",
    "# Aplicar función\n",
    "df['Total_ventas'] = df['Precio'].apply(total_ventas)\n",
    "print(df)\n",
    "\n",
    "# Clasificar productos como baratos, medios o caros\n",
    "def clasificar_productos(Precio):\n",
    "    if Precio < 20:\n",
    "        return \"Barato\"\n",
    "    elif Precio >= 21 and Precio < 50:\n",
    "        return \"Medio\"\n",
    "    else:\n",
    "        return \"Caro\"\n",
    "\n",
    "# Aplicar función personalizada a la columna 'precio' de df\n",
    "df['Clasificación_producto'] = df['Precio'].apply(clasificar_productos)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "behind-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "data = {'producto': ['Camisa', 'Pantalón', 'Zapatos'], 'precio': [20, 40, 50], 'cantidad': [10, 5, 8]}  \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "   \n",
    "df.to_csv('productos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efficient-rebecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'Conexion bd-pandas.ipynb', 'producto.csv', 'productos.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "iraqi-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base inicial:\n",
      "   id  producto  cantidad\n",
      "0   1    Camisa        10\n",
      "1   2  Pantalón         5\n",
      "2   3   Zapatos         8\n",
      "\n",
      "Carga completa (nueva base):\n",
      "   id  producto  cantidad\n",
      "0   1    Camisa        15\n",
      "1   2  Pantalón         7\n",
      "2   3   Zapatos        12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataset inicial\n",
    "data_base = {\n",
    "    'id': [1, 2, 3],\n",
    "    'producto': ['Camisa', 'Pantalón', 'Zapatos'],\n",
    "    'cantidad': [10, 5, 8]\n",
    "}\n",
    "df_base = pd.DataFrame(data_base)\n",
    "print(\"Base inicial:\")\n",
    "print(df_base)\n",
    "\n",
    "# Nueva carga completa\n",
    "data_nueva = {\n",
    "    'id': [1, 2, 3],\n",
    "    'producto': ['Camisa', 'Pantalón', 'Zapatos'],\n",
    "    'cantidad': [15, 7, 12]\n",
    "}\n",
    "df_completa = pd.DataFrame(data_nueva)\n",
    "print(\"\\nCarga completa (nueva base):\")\n",
    "print(df_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sustained-accused",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carga incremental (base actualizada):\n",
      "   id  producto  cantidad\n",
      "0   1    Camisa        10\n",
      "1   2  Pantalón         5\n",
      "2   3   Zapatos         8\n",
      "3   4  Sombrero         3\n",
      "4   5   Bufanda         2\n"
     ]
    }
   ],
   "source": [
    "# Nuevos datos incrementales\n",
    "data_incremental = {\n",
    "    'id': [4, 5],\n",
    "    'producto': ['Sombrero', 'Bufanda'],\n",
    "    'cantidad': [3, 2]\n",
    "}\n",
    "df_incremental = pd.DataFrame(data_incremental)\n",
    "df_actualizada = pd.concat([df_base, df_incremental], ignore_index=True)\n",
    "print(\"\\nCarga incremental (base actualizada):\")\n",
    "print(df_actualizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "champion-guess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base actualizada sin duplicados:\n",
      "   id  producto  cantidad\n",
      "0   1    Camisa        10\n",
      "1   2  Pantalón         5\n",
      "3   3   Zapatos        12\n",
      "4   4  Sombrero         3\n"
     ]
    }
   ],
   "source": [
    "# Datos iniciales\n",
    "data_base = {\n",
    "    'id': [1, 2, 3],\n",
    "    'producto': ['Camisa', 'Pantalón', 'Zapatos'],\n",
    "    'cantidad': [10, 5, 8]\n",
    "}\n",
    "df_base = pd.DataFrame(data_base)\n",
    "\n",
    "# Datos incrementales con duplicados\n",
    "data_incremental = {\n",
    "    'id': [3, 4],\n",
    "    'producto': ['Zapatos', 'Sombrero'],\n",
    "    'cantidad': [12, 3]\n",
    "}\n",
    "df_incremental = pd.DataFrame(data_incremental)\n",
    "\n",
    "# Concatenar y eliminar duplicados\n",
    "df_actualizada = pd.concat([df_base, df_incremental], ignore_index=True).drop_duplicates(subset=['id'], keep='last')\n",
    "print(\"\\nBase actualizada sin duplicados:\")\n",
    "print(df_actualizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar datos en CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "elementary-writer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos existentes combinados con los nuevos. Total registros después de la carga: 6\n",
      "Carga incremental realizada. Datos guardados en C:/Users/babst/Downloads/data_completo.csv\n"
     ]
    }
   ],
   "source": [
    "# Escenario inicial\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Rutas de los archivos\n",
    "archivo_nueva = 'C:/Users/babst/Downloads/data_nueva_1.csv'\n",
    "archivo_completo = 'C:/Users/babst/Downloads/data_completo.csv'\n",
    "\n",
    "# Cargar datos existentes y realizar carga incremental\n",
    "def carga_incremental(nuevos_datos_path, archivo_completo_path):\n",
    "    # Leer el archivo de datos nuevos\n",
    "    df_nuevos = pd.read_csv(nuevos_datos_path)\n",
    "\n",
    "    #Verificar si el arcivo completo ya existe\n",
    "    if os.path.exists(archivo_completo_path):\n",
    "        # Si el archivo existe, cargar los datos existentes\n",
    "        df_completo = pd.read_csv(archivo_completo_path)\n",
    "        # Concatenar los datos nuevos con los existentes\n",
    "        df_actualizado = pd.concat([df_completo, df_nuevos], ignore_index=True)\n",
    "        # Eliminar duplicados basados en la columna 'id' (o cualquier otra clave única)\n",
    "        df_actualizado = df_actualizado.drop_duplicates(subset=['id'], keep='last')\n",
    "        print(f\"Datos existentes combinados con los nuevos. Total registros después de la carga: {len(df_actualizado)}\")\n",
    "    else:\n",
    "        # Si el archivo no existe, usar solo los datos nuevos\n",
    "        df_actualizado = df_nuevos\n",
    "        print(f\"Archivo completo no existe. Usando solo los datos nuevos. Total registros: {len(df_actualizado)}\")\n",
    "    \n",
    "    # Guardar el archivo actualizado (si es nuevo o se combinó)\n",
    "    df_actualizado.to_csv(archivo_completo_path, index=False)\n",
    "    print(f\"Carga incremental realizada. Datos guardados en {archivo_completo_path}\")\n",
    "    \n",
    "# Ejecutar la función con datos nuevos (archivo completo puede no existir al principio)\n",
    "carga_incremental(archivo_nueva, archivo_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "obvious-tennessee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos existentes combinados con los nuevos. Total registros después de la carga: 6\n",
      "Carga incremental realizada. Datos guardados en C:/Users/babst/Downloads/data_completo.csv\n"
     ]
    }
   ],
   "source": [
    "#Carga incremental\n",
    "archivo_nueva = 'C:/Users/babst/Downloads/data_nueva_2.csv'     # Archivo con datos nuevos\n",
    "archivo_completo = 'C:/Users/babst/Downloads/data_completo.csv'  # Archivo donde se guardará la carga completa\n",
    "\n",
    "# Ejecutar la función con datos nuevos (archivo completo ya existe)\n",
    "carga_incremental(archivo_nueva, archivo_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "entertaining-approval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición Ropa:\n",
      "    id_venta  producto  cantidad categoria       fecha\n",
      "0         1    Camisa         2      Ropa  2024-05-01\n",
      "1         2  Pantalón         3      Ropa  2024-05-03\n",
      "3         4    Camisa         2      Ropa  2024-05-01\n",
      "4         5  Pantalón         0      Ropa  2024-05-02\n",
      "Partición Calzado:\n",
      "    id_venta producto  cantidad categoria       fecha\n",
      "2         3  Zapatos         1   Calzado  2024-05-02\n",
      "5         6  Zapatos         5   Calzado  2024-05-05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'id_venta': [1, 2, 3, 4, 5, 6],\n",
    "    'producto': ['Camisa', 'Pantalón', 'Zapatos', 'Camisa', 'Pantalón', 'Zapatos'],\n",
    "    'cantidad': [2, 3, 1, 2, 0, 5],\n",
    "    'categoria': ['Ropa', 'Ropa', 'Calzado', 'Ropa', 'Ropa', 'Calzado'],\n",
    "    'fecha': ['2024-05-01', '2024-05-03', '2024-05-02', '2024-05-01', '2024-05-02', '2024-05-05']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "ropa = df[df['categoria'] == 'Ropa']\n",
    "calzado = df[df['categoria'] == 'Calzado']\n",
    "\n",
    "print(\"Partición Ropa:\\n\", ropa)\n",
    "print(\"Partición Calzado:\\n\", calzado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dress-indication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mes 5:\n",
      "   id_venta  producto  cantidad categoria       fecha  mes\n",
      "0         1    Camisa         2      Ropa  2024-05-01    5\n",
      "1         2  Pantalón         3      Ropa  2024-05-03    5\n",
      "2         3   Zapatos         1   Calzado  2024-05-02    5\n",
      "3         4    Camisa         2      Ropa  2024-05-01    5\n",
      "4         5  Pantalón         0      Ropa  2024-05-02    5\n",
      "5         6   Zapatos         5   Calzado  2024-05-05    5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupación por fecha\n",
    "df['mes'] = pd.to_datetime(df['fecha']).dt.month\n",
    "particiones_por_mes = {mes: datos for mes, datos in df.groupby('mes')}\n",
    "\n",
    "for mes, datos in particiones_por_mes.items():\n",
    "    print(f\"Mes {mes}:\\n{datos}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "caroline-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición guardada en data_2023.csv\n",
      "Partición guardada en data_2024.csv\n"
     ]
    }
   ],
   "source": [
    "#Particionamiento de Datos - Ruta por default\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo data_completa.csv\n",
    "archivo_completo = 'C:/Users/babst/Downloads/data_completo.csv'\n",
    "\n",
    "# Función para particionar los datos por año\n",
    "def particion_por_año(archivo_completo_path):\n",
    "    # Leer el archivo completo\n",
    "    df = pd.read_csv(archivo_completo_path)\n",
    "    \n",
    "    # Particionar por año\n",
    "    for año, df_año in df.groupby('año'):\n",
    "        # Crear el nombre del archivo para cada año\n",
    "        nombre_archivo = f\"data_{año}.csv\"\n",
    "        # Guardar el DataFrame del año en un archivo CSV\n",
    "        df_año.to_csv(nombre_archivo, index=False)\n",
    "        print(f\"Partición guardada en {nombre_archivo}\")\n",
    "\n",
    "# Llamar a la función para particionar el archivo por año\n",
    "particion_por_año(archivo_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar datos en Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "united-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición guardada en C:/Users/babst/Downloads/data_2023.csv\n",
      "Partición guardada en C:/Users/babst/Downloads/data_2024.csv\n"
     ]
    }
   ],
   "source": [
    "#Particionamiento de Datos - Ruta personalizada\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ruta del archivo data_completa.csv\n",
    "archivo_completo = 'C:/Users/babst/Downloads/data_completo.csv'\n",
    "\n",
    "# Ruta de destino para los archivos particionados\n",
    "ruta_destino = 'C:/Users/babst/Downloads/'\n",
    "\n",
    "# Función para particionar los datos por año\n",
    "def particion_por_año(archivo_completo_path, ruta_destino):\n",
    "    # Leer el archivo completo\n",
    "    df = pd.read_csv(archivo_completo_path)\n",
    "    \n",
    "    # Particionar por año\n",
    "    for año, df_año in df.groupby('año'):\n",
    "        # Crear el nombre del archivo para cada año en la ruta de destino\n",
    "        nombre_archivo = os.path.join(ruta_destino, f\"data_{año}.csv\")\n",
    "        # Guardar el DataFrame del año en un archivo CSV\n",
    "        df_año.to_csv(nombre_archivo, index=False)\n",
    "        print(f\"Partición guardada en {nombre_archivo}\")\n",
    "\n",
    "# Llamar a la función para particionar el archivo por año\n",
    "particion_por_año(archivo_completo, ruta_destino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "inclusive-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generacion de los archivos!\n",
    "import pandas as pd\n",
    "\n",
    "# Crear el archivo data_nueva1.xlsx\n",
    "data_nueva_1 = {\n",
    "    'id': [1, 2, 3],\n",
    "    'nombre': ['Carla', 'Laura', 'Luis'],\n",
    "    'categoria': ['A', 'B', 'C'],\n",
    "    'ventas': [120, 170, 200],\n",
    "    'año': [2024, 2023, 2024]\n",
    "}\n",
    "\n",
    "df_datanueva_1 = pd.DataFrame(data_nueva_1)\n",
    "df_datanueva_1.to_excel('data_nueva1.xlsx', index = False)\n",
    "\n",
    "\n",
    "# Crear el archivo datanueva_2\n",
    "data_nueva_2 = {\n",
    "    'id': [4, 5, 6],\n",
    "    'nombre': ['Carlos', 'Lucia', 'Marcos'],\n",
    "    'categoria': ['A', 'B', 'C'],\n",
    "    'ventas': [210, 180, 250],\n",
    "    'año': [2024, 2023, 2024]\n",
    "}\n",
    "\n",
    "df_datanueva_2 = pd.DataFrame(data_nueva_2)\n",
    "df_datanueva_2.to_excel('data_nueva2.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "compliant-worship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos existentes combinados con los nuevos. Total registros después de la carga: 3\n",
      "Carga incremental realizada. Datos guardados en data_completa.xlsx\n"
     ]
    }
   ],
   "source": [
    "#Código para Carga Incremental con data 1:\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ruta de los archivos Excel\n",
    "archivo_nueva_1 = 'data_nueva1.xlsx'  # Archivo con datos nuevos (data 1)\n",
    "archivo_completo = 'data_completa.xlsx'  # Archivo donde se guardará la carga completa\n",
    "\n",
    "# Función para carga incremental con Excel\n",
    "def carga_incremental_excel(nuevos_datos_path, archivo_completo_path):\n",
    "    # Leer el archivo de datos nuevos\n",
    "    df_nuevos = pd.read_excel(nuevos_datos_path)\n",
    "    \n",
    "    # Verificar si el archivo completo ya existe\n",
    "    if os.path.exists(archivo_completo_path):\n",
    "        \n",
    "        # Leer el archivo completo (si existe) y combinarlo con los nuevos datos\n",
    "        df_completo = pd.read_excel(archivo_completo_path)\n",
    "        \n",
    "        # Concatenar los datos y eliminar duplicados basados en 'id'\n",
    "        df_actualizado = pd.concat([df_completo, df_nuevos]).drop_duplicates(subset=['id'], keep='last')\n",
    "        print(f\"Datos existentes combinados con los nuevos. Total registros después de la carga: {len(df_actualizado)}\")\n",
    "        \n",
    "    else:\n",
    "        # Si el archivo no existe, usar solo los datos nuevos\n",
    "        df_actualizado = df_nuevos\n",
    "        print(f\"Archivo completo no existe. Usando solo los datos nuevos. Total registros: {len(df_actualizado)}\")\n",
    "    \n",
    "    # Guardar el archivo actualizado en Excel\n",
    "    df_actualizado.to_excel(archivo_completo_path, index=False)\n",
    "    print(f\"Carga incremental realizada. Datos guardados en {archivo_completo_path}\")\n",
    "\n",
    "# Llamar a la función con el archivo de datos nuevos 1\n",
    "carga_incremental_excel(archivo_nueva_1, archivo_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "amended-proposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos existentes combinados con los nuevos. Total registros después de la carga: 6\n",
      "Carga incremental realizada. Datos guardados en data_completa.xlsx\n"
     ]
    }
   ],
   "source": [
    "#Código para Carga Incremental con data 2:\n",
    "archivo_nueva = 'data_nueva2.xlsx'     # Archivo con datos nuevos\n",
    "archivo_completo = 'data_completa.xlsx'  # Archivo donde se guardará la carga completa\n",
    "\n",
    "# Ejecutar la función con datos nuevos (archivo completo ya existe)\n",
    "carga_incremental_excel(archivo_nueva, archivo_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-swift",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

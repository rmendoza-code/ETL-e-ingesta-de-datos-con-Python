{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "advance-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "departmental-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conexion\n",
    "con = sqlite3.connect(\"C:/Users/babst/Downloads/nba_salary.sqlite\")\n",
    "\n",
    "#QUERY\n",
    "sql = \"SELECT name from sqlite_master where type = 'table'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "going-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear dt\n",
    "df = pd.read_sql_query(sql,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "solid-medline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name\n",
      "0  NBA_season1718_salary\n",
      "1          Seasons_Stats\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "twelve-technology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        X1          Player   Tm  season17_18\n",
      "0      1.0   Stephen Curry  GSW   34682550.0\n",
      "1      2.0    LeBron James  CLE   33285709.0\n",
      "2      3.0    Paul Millsap  DEN   31269231.0\n",
      "3      4.0  Gordon Hayward  BOS   29727900.0\n",
      "4      5.0   Blake Griffin  DET   29512900.0\n",
      "..     ...             ...  ...          ...\n",
      "568  569.0      Quinn Cook  NOP      25000.0\n",
      "569  570.0   Chris Johnson  HOU      25000.0\n",
      "570  571.0      Beno Udrih  DET      25000.0\n",
      "571  572.0   Joel Bolomboy  MIL      22248.0\n",
      "572  573.0    Jarell Eddie  CHI      17224.0\n",
      "\n",
      "[573 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_nba = pd.read_sql_query(\"Select * From NBA_season1718_salary\",con)\n",
    "print(df_nba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bizarre-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQLAlchemy\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lesbian-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conexion\n",
    "engine = db.create_engine('sqlite:///C:/Users/babst/Downloads/nba_salary.sqlite')\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bigger-cement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            X1    Year             Player  Pos   Age   Tm     G   GS      MP  \\\n",
      "0          0.0  1950.0    Curly Armstrong  G-F  31.0  FTW  63.0  NaN     NaN   \n",
      "1          1.0  1950.0       Cliff Barker   SG  29.0  INO  49.0  NaN     NaN   \n",
      "2          2.0  1950.0      Leo Barnhorst   SF  25.0  CHS  67.0  NaN     NaN   \n",
      "3          3.0  1950.0         Ed Bartels    F  24.0  TOT  15.0  NaN     NaN   \n",
      "4          4.0  1950.0         Ed Bartels    F  24.0  DNN  13.0  NaN     NaN   \n",
      "...        ...     ...                ...  ...   ...  ...   ...  ...     ...   \n",
      "24686  24686.0  2017.0        Cody Zeller   PF  24.0  CHO  62.0  NaN  1725.0   \n",
      "24687  24687.0  2017.0       Tyler Zeller    C  27.0  BOS  51.0  NaN   525.0   \n",
      "24688  24688.0  2017.0  Stephen Zimmerman    C  20.0  ORL  19.0  0.0   108.0   \n",
      "24689  24689.0  2017.0        Paul Zipser   SF  22.0  CHI  44.0  NaN   843.0   \n",
      "24690  24690.0  2017.0        Ivica Zubac    C  19.0  LAL  38.0  NaN   609.0   \n",
      "\n",
      "        PER  ...    FT%  ORB  DRB    TRB    AST  STL  BLK  TOV     PF    PTS  \n",
      "0       NaN  ...  0.705  NaN  NaN    NaN  176.0  NaN  NaN  NaN  217.0  458.0  \n",
      "1       NaN  ...  0.708  NaN  NaN    NaN  109.0  NaN  NaN  NaN   99.0  279.0  \n",
      "2       NaN  ...  0.698  NaN  NaN    NaN  140.0  NaN  NaN  NaN  192.0  438.0  \n",
      "3       NaN  ...  0.559  NaN  NaN    NaN   20.0  NaN  NaN  NaN   29.0   63.0  \n",
      "4       NaN  ...  0.548  NaN  NaN    NaN   20.0  NaN  NaN  NaN   27.0   59.0  \n",
      "...     ...  ...    ...  ...  ...    ...    ...  ...  ...  ...    ...    ...  \n",
      "24686  16.7  ...  0.679  NaN  NaN  405.0   99.0  NaN  NaN  NaN  189.0  639.0  \n",
      "24687  13.0  ...  0.564  NaN  NaN  124.0   42.0  NaN  NaN  NaN   61.0  178.0  \n",
      "24688   7.3  ...  0.600  NaN  NaN   35.0    4.0  NaN  NaN  NaN   17.0   23.0  \n",
      "24689   6.9  ...  0.775  NaN  NaN  125.0   36.0  NaN  NaN  NaN   78.0  240.0  \n",
      "24690  17.0  ...  0.653  NaN  NaN  159.0   30.0  NaN  NaN  NaN   66.0  284.0  \n",
      "\n",
      "[24691 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_nba_sqlalchemy = pd.read_sql_query(\"Select * from Seasons_Stats\",connection)\n",
    "print(df_nba_sqlalchemy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "distinguished-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consultas!\n",
    "df_Curly = pd.read_sql_query(\"SELECT * from Seasons_Stats where Player = 'Curly Armstrong'\",connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "central-ocean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>Year</th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>PER</th>\n",
       "      <th>...</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>Curly Armstrong</td>\n",
       "      <td>G-F</td>\n",
       "      <td>31.0</td>\n",
       "      <td>FTW</td>\n",
       "      <td>63.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>217.0</td>\n",
       "      <td>458.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>314.0</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>Curly Armstrong</td>\n",
       "      <td>G-F</td>\n",
       "      <td>32.0</td>\n",
       "      <td>FTW</td>\n",
       "      <td>38.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>89.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>97.0</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X1    Year           Player  Pos   Age   Tm     G    GS    MP   PER  \\\n",
       "0    0.0  1950.0  Curly Armstrong  G-F  31.0  FTW  63.0  None  None  None   \n",
       "1  314.0  1951.0  Curly Armstrong  G-F  32.0  FTW  38.0  None  None  None   \n",
       "\n",
       "   ...    FT%   ORB   DRB   TRB    AST   STL   BLK   TOV     PF    PTS  \n",
       "0  ...  0.705  None  None   NaN  176.0  None  None  None  217.0  458.0  \n",
       "1  ...  0.644  None  None  89.0   77.0  None  None  None   97.0  202.0  \n",
       "\n",
       "[2 rows x 53 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Curly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deluxe-summit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510.1163499025341"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prom_PTS = df_nba_sqlalchemy['PTS'].mean()\n",
    "prom_PTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "technological-helen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    100\n",
      "1    200\n",
      "2    300\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "serie = pd.Series([100, 200, 300])\n",
    "print(serie)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "square-breath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Product  Sales\n",
      "0       A    100\n",
      "1       B    200\n"
     ]
    }
   ],
   "source": [
    "data = {'Product': ['A', 'B'], 'Sales': [100, 200]}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "attractive-design",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   new_column\n",
      "0         100\n",
      "1         200\n",
      "2         300\n"
     ]
    }
   ],
   "source": [
    "df_nuevo = serie.to_frame(name='new_column')\n",
    "print(df_nuevo)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "peripheral-wellington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1        0\n",
       "Year      0\n",
       "Player    0\n",
       "Pos       0\n",
       "Age       0\n",
       "Tm        0\n",
       "G         0\n",
       "GS        2\n",
       "MP        2\n",
       "PER       2\n",
       "TS%       0\n",
       "3PAr      2\n",
       "FTr       0\n",
       "ORB%      2\n",
       "DRB%      2\n",
       "TRB%      2\n",
       "AST%      2\n",
       "STL%      2\n",
       "BLK%      2\n",
       "TOV%      2\n",
       "USG%      2\n",
       "blanl     2\n",
       "OWS       0\n",
       "DWS       0\n",
       "WS        0\n",
       "WS/48     2\n",
       "blank2    2\n",
       "OBPM      2\n",
       "DBPM      2\n",
       "BPM       2\n",
       "VORP      2\n",
       "FG        0\n",
       "FGA       0\n",
       "FG%       0\n",
       "3P        2\n",
       "3PA       2\n",
       "3P%       2\n",
       "2P        0\n",
       "2PA       0\n",
       "2P%       0\n",
       "eFG%      0\n",
       "FT        0\n",
       "FTA       0\n",
       "FT%       0\n",
       "ORB       2\n",
       "DRB       2\n",
       "TRB       1\n",
       "AST       0\n",
       "STL       2\n",
       "BLK       2\n",
       "TOV       2\n",
       "PF        0\n",
       "PTS       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Curly.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cutting-symposium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicados = df_Curly.duplicated()\n",
    "print(duplicados.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "intensive-baking",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date_column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1ab67321b2b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date_column'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalidate_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date_column'"
     ]
    }
   ],
   "source": [
    "df['date_column'].apply(lambda x: validate_date(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Leer Archivo CSV generado\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/babst/Downloads/datos_ejemplo.csv\")\n",
    "\n",
    "# Mostrar el df original\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Manejo de valores nulos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum()) #Verificar valores nulos\n",
    "\n",
    "#3 Eliminar las filas que contienen valores nulos\n",
    "df_sin_nulos = df.dropna()\n",
    "\n",
    "#4 Rellenar los valores nulos con un valor especÃ­fico (por ejemplo, 0 o \"Desconocido\")\n",
    "df_rellenado = df.fillna({'Salario': 0, 'Nombre': 'Desconocido'})\n",
    "\n",
    "print(\"\\nDataFrame despuÃ©s de eliminar nulos:\")\n",
    "print(df_sin_nulos)\n",
    "\n",
    "print(\"\\nDataFrame despuÃ©s de rellenar nulos:\")\n",
    "print(df_rellenado) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 DetecciÃ³ny CorreciÃ³n de errores en los tipos de datos\n",
    "\n",
    "# Intentar convertir la columna 'Edad' a numÃ©rica\n",
    "df['Edad'] = pd.to_numeric(df['Edad'], errors = 'coerce')\n",
    "\n",
    "# Ver los tipos de datos de las columnas antes de la correciÃ³n\n",
    "print(df.dtypes)\n",
    "\n",
    "# Corregir valores no numÃ©ricos en 'Edad' (por ejemplo, convertir a NaN)\n",
    "df['Edad'] = df['Edad'].fillna(df['Edad'].mean()) #Rellenar NaN con el promedio de la columna\n",
    "\n",
    "# Ver los tipos de datos despuÃ©s de la correciÃ³n\n",
    "print(df.dtypes)\n",
    "\n",
    "# Ver el df corregido\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 ConversiÃ³n de variables categÃ³ricas en nÃºmeros\n",
    "\n",
    "df['Genero'] = df['Genero'].map({'F': 0, 'M': 1}) \n",
    "\n",
    "# Usar la tÃ©cnica de 'get_dummies' para convertir 'Departamento' en variables dummy\n",
    "df = pd.get_dummies(df, columns = ['Departamento'], drop_first = True)\n",
    "\n",
    "# Ver el DF final\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = df[df['Edad'] > 35]\n",
    "print(df_filtrado) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seleccion = df[['Edad', 'Salario']]\n",
    "print(df_seleccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nueva_columna'] = df['Edad'] * df['Salario']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'producto': ['A', 'B', 'C'],\n",
    "    'cantidad': [3, 0, 5],\n",
    "    'precio_unitario': [10, 20, 15]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = df[df['cantidad'] > 1]\n",
    "print(df_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seleccion = df_filtrado[['producto', 'cantidad']]\n",
    "print(df_seleccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado['total_venta'] = df_filtrado['cantidad'] * df_filtrado['precio_unitario']\n",
    "print(df_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar y Resumir Datos\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'producto': ['A', 'B', 'A', 'B'],\n",
    "    'ventas': [10, 20, 30, 40]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "grupo = df.groupby('producto').agg({'ventas': ['sum', 'mean']})\n",
    "print(grupo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#1. Importar archivo CSV de empleados  y bonificaciones\n",
    "df_empleados = pd.read_csv(\"C:/Users/babst/Downloads/empleados.csv\")\n",
    "df_bonificaciones = pd.read_csv(\"C:/Users/babst/Downloads/bonificaciones.csv\")\n",
    "\n",
    "#2. Mostrar DataFrames\n",
    "print(df_empleados)\n",
    "print(df_bonificaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Aplicar funciones personalizadas\n",
    "\n",
    "# Definir una funciÃ³n personalizada para calcular el salario anualizado\n",
    "def salario_anual(salario):\n",
    "    return salario * 12\n",
    "\n",
    "# Aplicar la funciÃ³n personalizada a la columna 'Salario'\n",
    "df_empleados['Salario_anual'] = df_empleados['Salario'].apply(salario_anual)\n",
    "print(df_empleados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Uso de apply()  para aplicar funciones mÃ¡s complejas\n",
    "\n",
    "# Crear una funciÃ³n que calculesi un empleado tiene mÃ¡s de 5 aÃ±os de antiguÃ¼edad\n",
    "def antiguedad_5anos(fecha_ingreso):\n",
    "    today = pd.to_datetime('today')\n",
    "    antiguedad = today - pd.to_datetime(fecha_ingreso)\n",
    "    return antiguedad.days / 365 > 5\n",
    "\n",
    "df_empleados['Antiguedad_mayor_5'] = df_empleados['Fecha_Ingreso'].apply(antiguedad_5anos)\n",
    "print(df_empleados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Crear tablas pivote (Pivot Tables)\n",
    "\n",
    "# Crear una tabla pivote que muestre el salario promedio por departamento\n",
    "pivot_departamento = df_empleados.pivot_table(values = 'Salario', index = 'Departamento', aggfunc = 'mean')\n",
    "\n",
    "print(pivot_departamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Merge y Join para combinar DataFrames\n",
    "\n",
    "# Realizar un merge entre los dos DataFrames usando la columna 'ID_empleado'\n",
    "df_completo = pd.merge(df_empleados, df_bonificaciones, on = 'ID_Empleado', how = 'left')\n",
    "\n",
    "print(df_completo)\n",
    " \n",
    "    \n",
    "# Usar Join:    \n",
    "# Crear dataframe adicional de departamentos con la informaciÃ³n de ubicaciÃ³n \n",
    "data_departamentos = {\n",
    "    'Departamento': ['Ventas', 'TI', 'Marketing'],\n",
    "    'UbicaciÃ³n': ['Madrid','Barcelona', 'Valencia']\n",
    "}\n",
    "\n",
    "df_departamentos = pd.DataFrame(data_departamentos)\n",
    "\n",
    "# Establecer 'Departamento' como Ã­ndice del DataFrame de departamentos\n",
    "df_departamentos.set_index('Departamento', inplace = True)\n",
    "\n",
    "#Realizar un join entre el DataFrame de empleados y el de departamentos usando la columna 'Departamento'\n",
    "df_join = df_empleados.set_index('Departamento').join(df_departamentos)\n",
    "\n",
    "print(df_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "silent-torture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Producto  Precio  Cantidad\n",
      "0    Camisa    20.0         2\n",
      "1  PantalÃ³n    40.0         3\n",
      "2   Zapatos    50.0         1\n",
      "3  Sudarera    60.0         2\n",
      "4  Sombrero    15.0         5\n",
      "   Producto  Precio  Cantidad  Total_ventas\n",
      "0    Camisa    20.0         2          40.0\n",
      "1  PantalÃ³n    40.0         3         120.0\n",
      "2   Zapatos    50.0         1          50.0\n",
      "3  Sudarera    60.0         2         120.0\n",
      "4  Sombrero    15.0         5          75.0\n",
      "   Producto  Precio  Cantidad  Total_ventas ClasificaciÃ³n_producto\n",
      "0    Camisa    20.0         2          40.0                   Caro\n",
      "1  PantalÃ³n    40.0         3         120.0                  Medio\n",
      "2   Zapatos    50.0         1          50.0                   Caro\n",
      "3  Sudarera    60.0         2         120.0                   Caro\n",
      "4  Sombrero    15.0         5          75.0                 Barato\n"
     ]
    }
   ],
   "source": [
    "# Crear DataFrame \n",
    "data = {\n",
    "    'Producto': ['Camisa', 'PantalÃ³n', 'Zapatos', 'Sudarera', 'Sombrero'],\n",
    "    'Precio': [20.0, 40.0 ,50.0 ,60.0 ,15.0],\n",
    "    'Cantidad': [2, 3, 1, 2, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "def antiguedad_5anos(fecha_ingreso):\n",
    "    today = pd.to_datetime('today')\n",
    "    antiguedad = today - pd.to_datetime(fecha_ingreso)\n",
    "    return antiguedad.days / 365 > 5\n",
    "\n",
    "# Definir funciÃ³n\n",
    "def total_ventas(precio):\n",
    "    indice = df[df['Precio'] == precio].index[0]\n",
    "    cantidad = df.loc[indice, 'Cantidad']\n",
    "    return precio * cantidad\n",
    "\n",
    "# Aplicar funciÃ³n\n",
    "df['Total_ventas'] = df['Precio'].apply(total_ventas)\n",
    "print(df)\n",
    "\n",
    "# Clasificar productos como baratos, medios o caros\n",
    "def clasificar_productos(Precio):\n",
    "    if Precio < 20:\n",
    "        return \"Barato\"\n",
    "    elif Precio >= 21 and Precio < 50:\n",
    "        return \"Medio\"\n",
    "    else:\n",
    "        return \"Caro\"\n",
    "\n",
    "# Aplicar funciÃ³n personalizada a la columna 'precio' de df\n",
    "df['ClasificaciÃ³n_producto'] = df['Precio'].apply(clasificar_productos)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "behind-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "data = {'producto': ['Camisa', 'PantalÃ³n', 'Zapatos'], 'precio': [20, 40, 50], 'cantidad': [10, 5, 8]}  \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "   \n",
    "df.to_csv('productos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efficient-rebecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'Conexion bd-pandas.ipynb', 'producto.csv', 'productos.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "iraqi-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base inicial:\n",
      "   id  producto  cantidad\n",
      "0   1    Camisa        10\n",
      "1   2  PantalÃ³n         5\n",
      "2   3   Zapatos         8\n",
      "\n",
      "Carga completa (nueva base):\n",
      "   id  producto  cantidad\n",
      "0   1    Camisa        15\n",
      "1   2  PantalÃ³n         7\n",
      "2   3   Zapatos        12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataset inicial\n",
    "data_base = {\n",
    "    'id': [1, 2, 3],\n",
    "    'producto': ['Camisa', 'PantalÃ³n', 'Zapatos'],\n",
    "    'cantidad': [10, 5, 8]\n",
    "}\n",
    "df_base = pd.DataFrame(data_base)\n",
    "print(\"Base inicial:\")\n",
    "print(df_base)\n",
    "\n",
    "# Nueva carga completa\n",
    "data_nueva = {\n",
    "    'id': [1, 2, 3],\n",
    "    'producto': ['Camisa', 'PantalÃ³n', 'Zapatos'],\n",
    "    'cantidad': [15, 7, 12]\n",
    "}\n",
    "df_completa = pd.DataFrame(data_nueva)\n",
    "print(\"\\nCarga completa (nueva base):\")\n",
    "print(df_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sustained-accused",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carga incremental (base actualizada):\n",
      "   id  producto  cantidad\n",
      "0   1    Camisa        10\n",
      "1   2  PantalÃ³n         5\n",
      "2   3   Zapatos         8\n",
      "3   4  Sombrero         3\n",
      "4   5   Bufanda         2\n"
     ]
    }
   ],
   "source": [
    "# Nuevos datos incrementales\n",
    "data_incremental = {\n",
    "    'id': [4, 5],\n",
    "    'producto': ['Sombrero', 'Bufanda'],\n",
    "    'cantidad': [3, 2]\n",
    "}\n",
    "df_incremental = pd.DataFrame(data_incremental)\n",
    "df_actualizada = pd.concat([df_base, df_incremental], ignore_index=True)\n",
    "print(\"\\nCarga incremental (base actualizada):\")\n",
    "print(df_actualizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "champion-guess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base actualizada sin duplicados:\n",
      "   id  producto  cantidad\n",
      "0   1    Camisa        10\n",
      "1   2  PantalÃ³n         5\n",
      "3   3   Zapatos        12\n",
      "4   4  Sombrero         3\n"
     ]
    }
   ],
   "source": [
    "# Datos iniciales\n",
    "data_base = {\n",
    "    'id': [1, 2, 3],\n",
    "    'producto': ['Camisa', 'PantalÃ³n', 'Zapatos'],\n",
    "    'cantidad': [10, 5, 8]\n",
    "}\n",
    "df_base = pd.DataFrame(data_base)\n",
    "\n",
    "# Datos incrementales con duplicados\n",
    "data_incremental = {\n",
    "    'id': [3, 4],\n",
    "    'producto': ['Zapatos', 'Sombrero'],\n",
    "    'cantidad': [12, 3]\n",
    "}\n",
    "df_incremental = pd.DataFrame(data_incremental)\n",
    "\n",
    "# Concatenar y eliminar duplicados\n",
    "df_actualizada = pd.concat([df_base, df_incremental], ignore_index=True).drop_duplicates(subset=['id'], keep='last')\n",
    "print(\"\\nBase actualizada sin duplicados:\")\n",
    "print(df_actualizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar datos en CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "elementary-writer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos existentes combinados con los nuevos. Total registros despuÃ©s de la carga: 6\n",
      "Carga incremental realizada. Datos guardados en C:/Users/babst/Downloads/data_completo.csv\n"
     ]
    }
   ],
   "source": [
    "# Escenario inicial\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Rutas de los archivos\n",
    "archivo_nueva = 'C:/Users/babst/Downloads/data_nueva_1.csv'\n",
    "archivo_completo = 'C:/Users/babst/Downloads/data_completo.csv'\n",
    "\n",
    "# Cargar datos existentes y realizar carga incremental\n",
    "def carga_incremental(nuevos_datos_path, archivo_completo_path):\n",
    "    # Leer el archivo de datos nuevos\n",
    "    df_nuevos = pd.read_csv(nuevos_datos_path)\n",
    "\n",
    "    #Verificar si el arcivo completo ya existe\n",
    "    if os.path.exists(archivo_completo_path):\n",
    "        # Si el archivo existe, cargar los datos existentes\n",
    "        df_completo = pd.read_csv(archivo_completo_path)\n",
    "        # Concatenar los datos nuevos con los existentes\n",
    "        df_actualizado = pd.concat([df_completo, df_nuevos], ignore_index=True)\n",
    "        # Eliminar duplicados basados en la columna 'id' (o cualquier otra clave Ãºnica)\n",
    "        df_actualizado = df_actualizado.drop_duplicates(subset=['id'], keep='last')\n",
    "        print(f\"Datos existentes combinados con los nuevos. Total registros despuÃ©s de la carga: {len(df_actualizado)}\")\n",
    "    else:\n",
    "        # Si el archivo no existe, usar solo los datos nuevos\n",
    "        df_actualizado = df_nuevos\n",
    "        print(f\"Archivo completo no existe. Usando solo los datos nuevos. Total registros: {len(df_actualizado)}\")\n",
    "    \n",
    "    # Guardar el archivo actualizado (si es nuevo o se combinÃ³)\n",
    "    df_actualizado.to_csv(archivo_completo_path, index=False)\n",
    "    print(f\"Carga incremental realizada. Datos guardados en {archivo_completo_path}\")\n",
    "    \n",
    "# Ejecutar la funciÃ³n con datos nuevos (archivo completo puede no existir al principio)\n",
    "carga_incremental(archivo_nueva, archivo_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "obvious-tennessee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos existentes combinados con los nuevos. Total registros despuÃ©s de la carga: 6\n",
      "Carga incremental realizada. Datos guardados en C:/Users/babst/Downloads/data_completo.csv\n"
     ]
    }
   ],
   "source": [
    "#Carga incremental\n",
    "archivo_nueva = 'C:/Users/babst/Downloads/data_nueva_2.csv'     # Archivo con datos nuevos\n",
    "archivo_completo = 'C:/Users/babst/Downloads/data_completo.csv'  # Archivo donde se guardarÃ¡ la carga completa\n",
    "\n",
    "# Ejecutar la funciÃ³n con datos nuevos (archivo completo ya existe)\n",
    "carga_incremental(archivo_nueva, archivo_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "entertaining-approval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParticiÃ³n Ropa:\n",
      "    id_venta  producto  cantidad categoria       fecha\n",
      "0         1    Camisa         2      Ropa  2024-05-01\n",
      "1         2  PantalÃ³n         3      Ropa  2024-05-03\n",
      "3         4    Camisa         2      Ropa  2024-05-01\n",
      "4         5  PantalÃ³n         0      Ropa  2024-05-02\n",
      "ParticiÃ³n Calzado:\n",
      "    id_venta producto  cantidad categoria       fecha\n",
      "2         3  Zapatos         1   Calzado  2024-05-02\n",
      "5         6  Zapatos         5   Calzado  2024-05-05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'id_venta': [1, 2, 3, 4, 5, 6],\n",
    "    'producto': ['Camisa', 'PantalÃ³n', 'Zapatos', 'Camisa', 'PantalÃ³n', 'Zapatos'],\n",
    "    'cantidad': [2, 3, 1, 2, 0, 5],\n",
    "    'categoria': ['Ropa', 'Ropa', 'Calzado', 'Ropa', 'Ropa', 'Calzado'],\n",
    "    'fecha': ['2024-05-01', '2024-05-03', '2024-05-02', '2024-05-01', '2024-05-02', '2024-05-05']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "ropa = df[df['categoria'] == 'Ropa']\n",
    "calzado = df[df['categoria'] == 'Calzado']\n",
    "\n",
    "print(\"ParticiÃ³n Ropa:\\n\", ropa)\n",
    "print(\"ParticiÃ³n Calzado:\\n\", calzado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dress-indication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mes 5:\n",
      "   id_venta  producto  cantidad categoria       fecha  mes\n",
      "0         1    Camisa         2      Ropa  2024-05-01    5\n",
      "1         2  PantalÃ³n         3      Ropa  2024-05-03    5\n",
      "2         3   Zapatos         1   Calzado  2024-05-02    5\n",
      "3         4    Camisa         2      Ropa  2024-05-01    5\n",
      "4         5  PantalÃ³n         0      Ropa  2024-05-02    5\n",
      "5         6   Zapatos         5   Calzado  2024-05-05    5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AgrupaciÃ³n por fecha\n",
    "df['mes'] = pd.to_datetime(df['fecha']).dt.month\n",
    "particiones_por_mes = {mes: datos for mes, datos in df.groupby('mes')}\n",
    "\n",
    "for mes, datos in particiones_por_mes.items():\n",
    "    print(f\"Mes {mes}:\\n{datos}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "caroline-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParticiÃ³n guardada en data_2023.csv\n",
      "ParticiÃ³n guardada en data_2024.csv\n"
     ]
    }
   ],
   "source": [
    "#Particionamiento de Datos - Ruta por default\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo data_completa.csv\n",
    "archivo_completo = 'C:/Users/babst/Downloads/data_completo.csv'\n",
    "\n",
    "# FunciÃ³n para particionar los datos por aÃ±o\n",
    "def particion_por_aÃ±o(archivo_completo_path):\n",
    "    # Leer el archivo completo\n",
    "    df = pd.read_csv(archivo_completo_path)\n",
    "    \n",
    "    # Particionar por aÃ±o\n",
    "    for aÃ±o, df_aÃ±o in df.groupby('aÃ±o'):\n",
    "        # Crear el nombre del archivo para cada aÃ±o\n",
    "        nombre_archivo = f\"data_{aÃ±o}.csv\"\n",
    "        # Guardar el DataFrame del aÃ±o en un archivo CSV\n",
    "        df_aÃ±o.to_csv(nombre_archivo, index=False)\n",
    "        print(f\"ParticiÃ³n guardada en {nombre_archivo}\")\n",
    "\n",
    "# Llamar a la funciÃ³n para particionar el archivo por aÃ±o\n",
    "particion_por_aÃ±o(archivo_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar datos en Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "united-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParticiÃ³n guardada en C:/Users/babst/Downloads/data_2023.csv\n",
      "ParticiÃ³n guardada en C:/Users/babst/Downloads/data_2024.csv\n"
     ]
    }
   ],
   "source": [
    "#Particionamiento de Datos - Ruta personalizada\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ruta del archivo data_completa.csv\n",
    "archivo_completo = 'C:/Users/babst/Downloads/data_completo.csv'\n",
    "\n",
    "# Ruta de destino para los archivos particionados\n",
    "ruta_destino = 'C:/Users/babst/Downloads/'\n",
    "\n",
    "# FunciÃ³n para particionar los datos por aÃ±o\n",
    "def particion_por_aÃ±o(archivo_completo_path, ruta_destino):\n",
    "    # Leer el archivo completo\n",
    "    df = pd.read_csv(archivo_completo_path)\n",
    "    \n",
    "    # Particionar por aÃ±o\n",
    "    for aÃ±o, df_aÃ±o in df.groupby('aÃ±o'):\n",
    "        # Crear el nombre del archivo para cada aÃ±o en la ruta de destino\n",
    "        nombre_archivo = os.path.join(ruta_destino, f\"data_{aÃ±o}.csv\")\n",
    "        # Guardar el DataFrame del aÃ±o en un archivo CSV\n",
    "        df_aÃ±o.to_csv(nombre_archivo, index=False)\n",
    "        print(f\"ParticiÃ³n guardada en {nombre_archivo}\")\n",
    "\n",
    "# Llamar a la funciÃ³n para particionar el archivo por aÃ±o\n",
    "particion_por_aÃ±o(archivo_completo, ruta_destino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "inclusive-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generacion de los archivos!\n",
    "import pandas as pd\n",
    "\n",
    "# Crear el archivo data_nueva1.xlsx\n",
    "data_nueva_1 = {\n",
    "    'id': [1, 2, 3],\n",
    "    'nombre': ['Carla', 'Laura', 'Luis'],\n",
    "    'categoria': ['A', 'B', 'C'],\n",
    "    'ventas': [120, 170, 200],\n",
    "    'aÃ±o': [2024, 2023, 2024]\n",
    "}\n",
    "\n",
    "df_datanueva_1 = pd.DataFrame(data_nueva_1)\n",
    "df_datanueva_1.to_excel('data_nueva1.xlsx', index = False)\n",
    "\n",
    "\n",
    "# Crear el archivo datanueva_2\n",
    "data_nueva_2 = {\n",
    "    'id': [4, 5, 6],\n",
    "    'nombre': ['Carlos', 'Lucia', 'Marcos'],\n",
    "    'categoria': ['A', 'B', 'C'],\n",
    "    'ventas': [210, 180, 250],\n",
    "    'aÃ±o': [2024, 2023, 2024]\n",
    "}\n",
    "\n",
    "df_datanueva_2 = pd.DataFrame(data_nueva_2)\n",
    "df_datanueva_2.to_excel('data_nueva2.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "compliant-worship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos existentes combinados con los nuevos. Total registros despuÃ©s de la carga: 3\n",
      "Carga incremental realizada. Datos guardados en data_completa.xlsx\n"
     ]
    }
   ],
   "source": [
    "#CÃ³digo para Carga Incremental con data 1:\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ruta de los archivos Excel\n",
    "archivo_nueva_1 = 'data_nueva1.xlsx'  # Archivo con datos nuevos (data 1)\n",
    "archivo_completo = 'data_completa.xlsx'  # Archivo donde se guardarÃ¡ la carga completa\n",
    "\n",
    "# FunciÃ³n para carga incremental con Excel\n",
    "def carga_incremental_excel(nuevos_datos_path, archivo_completo_path):\n",
    "    # Leer el archivo de datos nuevos\n",
    "    df_nuevos = pd.read_excel(nuevos_datos_path)\n",
    "    \n",
    "    # Verificar si el archivo completo ya existe\n",
    "    if os.path.exists(archivo_completo_path):\n",
    "        \n",
    "        # Leer el archivo completo (si existe) y combinarlo con los nuevos datos\n",
    "        df_completo = pd.read_excel(archivo_completo_path)\n",
    "        \n",
    "        # Concatenar los datos y eliminar duplicados basados en 'id'\n",
    "        df_actualizado = pd.concat([df_completo, df_nuevos]).drop_duplicates(subset=['id'], keep='last')\n",
    "        print(f\"Datos existentes combinados con los nuevos. Total registros despuÃ©s de la carga: {len(df_actualizado)}\")\n",
    "        \n",
    "    else:\n",
    "        # Si el archivo no existe, usar solo los datos nuevos\n",
    "        df_actualizado = df_nuevos\n",
    "        print(f\"Archivo completo no existe. Usando solo los datos nuevos. Total registros: {len(df_actualizado)}\")\n",
    "    \n",
    "    # Guardar el archivo actualizado en Excel\n",
    "    df_actualizado.to_excel(archivo_completo_path, index=False)\n",
    "    print(f\"Carga incremental realizada. Datos guardados en {archivo_completo_path}\")\n",
    "\n",
    "# Llamar a la funciÃ³n con el archivo de datos nuevos 1\n",
    "carga_incremental_excel(archivo_nueva_1, archivo_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "amended-proposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos existentes combinados con los nuevos. Total registros despuÃ©s de la carga: 6\n",
      "Carga incremental realizada. Datos guardados en data_completa.xlsx\n"
     ]
    }
   ],
   "source": [
    "#CÃ³digo para Carga Incremental con data 2:\n",
    "archivo_nueva = 'data_nueva2.xlsx'     # Archivo con datos nuevos\n",
    "archivo_completo = 'data_completa.xlsx'  # Archivo donde se guardarÃ¡ la carga completa\n",
    "\n",
    "# Ejecutar la funciÃ³n con datos nuevos (archivo completo ya existe)\n",
    "carga_incremental_excel(archivo_nueva, archivo_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-swift",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
